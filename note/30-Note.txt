数据湖


RDBMS：存储 查询
数仓： 统计分析
NoSQL： 存储  查询
Kafka： 消息队列
流式计算：实时
离线计算：批


信息孤岛
	信息可能分散在各个地方
	无法进行集中的或者统一的存储
	==> 数仓

非结构化的数据
	结构化：具备使用Schema描述的
	非结构化：无法用Schema定义的数据
	半结构化：	

保留原始数据
	

数据湖定义
https://www.munichre.com/topics-online/en/digitalisation/managing-major-losses-digitally.html
	保留原始数据
	可以存储各种类型的数据
	可以支持各种的数据分析
	集中存储


          数据集市  vs  数仓  vs 数据湖  
应用范围
数据类型
存储规模
数据应用
开发周期








业界数据湖产品/框架常用的有哪些：
Hudi：
	hudi.apache.org
	
Delta：
	https://delta.io

IceBerg：
	http://iceberg.incubator.apache.org


















Apache Hudi
	Hadoop Upserts Delete and Incremental  流数据湖平台


2015  增量处理
2016  Uber
2017  开源
2019  进入孵化
2020  毕业  成为顶线项目    xxx.apache.org   github.com/apache/xxx
2021
2022
2023

轻量级的一个框架
	Hadoop集群：
	Spark/Flink：  更多的并不是去使用standalone模式，而是使用YARN、K8S模式
	Hudi：编译出来  一堆的jar包  不需要集群的  也是作业个客户端

Flink 
	EventTime： 数据延迟、乱序问题
	ProcessingTime：


在Hudi中有两种数据的存储
	列式存储：Parquet
	行式存储：avro  log


# Spark 3.3
spark-shell \
  --packages org.apache.hudi:hudi-spark3.3-bundle_2.12:0.12.1 \
  --conf 'spark.serializer=org.apache.spark.serializer.KryoSerializer' \
  --conf 'spark.sql.catalog.spark_catalog=org.apache.spark.sql.hudi.catalog.HoodieCatalog' \
  --conf 'spark.sql.extensions=org.apache.spark.sql.hudi.HoodieSparkSessionExtension'


import org.apache.hudi.QuickstartUtils._
import scala.collection.JavaConversions._
import org.apache.spark.sql.SaveMode._
import org.apache.hudi.DataSourceReadOptions._
import org.apache.hudi.DataSourceWriteOptions._
import org.apache.hudi.config.HoodieWriteConfig._
import org.apache.hudi.common.model.HoodieRecord

val tableName = "hudi_trips_cow"
val basePath = "hdfs://hadoop000:8020/pk/hudi_trips_cow"
val dataGen = new DataGenerator


spark.read.
format("hudi").
option("as.of.instant", "20230102211853205").
load(basePath).show




Hive：
	数据：分布式文件系统
		非分区表：${path}/pk/emp/xx.txt
		分区表： ${path}/access/day=yyyyMMdd/1.txt
	元数据：MySQL


1）数据存在指定的bashpath下
2）表可以划分成多个分区
3）在每个分区中，文件被组织成文件组， 唯一的id
4）每个组包含文件片  <=  版本
5）每个文件片包含
	base file (.parquet)
	log files (.log.*)
6) MVCC


COW: parquet
MOR: parquet+log



Hudi中支持的两种表类型：
Copy On Write
	写时拷贝：在写入时进行文件的拷贝，会有写入的延迟
	cow
	只有数据数据/基础文件，没有增量日志文件
	parquet
	每一个批次写入都将会创建相应数据文件的新版本(旧版本的数据+本次进来的数据)
	整个parquet文件中的数据 是全量数据
	不需要compaction

Merge On Read 
	读时合并：
	mor
	基础文件parquet+增量日志文件avro
	读取时实时合并基础文件以及各自的增量日志文件，会有读取的延迟
	compaction：异步、同步      时间  记录
	Q：MOR格式一定有Parquet文件吗？



CREATE TABLE t1(
  uuid VARCHAR(20) PRIMARY KEY NOT ENFORCED,
  name VARCHAR(10),
  age INT,
  ts TIMESTAMP(3),
  `partition` VARCHAR(20)
)
PARTITIONED BY (`partition`)
WITH (
  'connector' = 'hudi',
  'path' = 'hdfs://hadoop000:8020/pk/hudi/t1',
  'table.type' = 'MERGE_ON_READ'
);



case：
Flink SQL对接Kafka数据

1) 启动zk
2）启动kafka
3）创建一个topic
4）创建一个Flink SQL表 对接Kafka数据
5) 发送数据到topic




create table kafka_access_source(
user_id string,
domain string
)with(
'connector'='kafka',
'topic'='test1',
'properties.bootstrap.servers'='hadoop000:9092',
'properties.group.id'='pk',
'scan.startup.mode'='latest-offset',
'format'='csv'
);

扩展：接入kafka的数据 落入hudi

需求分析
1）创建一个Flink SQL表，对接Kafka数据   <== 源表
2) 创建一个Flink SQL表，对接Hudi       <== 目标表
3) insert into 目标表 select ...... from 源表


造数据：写一段Mock代码，发送数据到指定的topic中: ordertopic
订单字段：
	订单编号：id        yyyyMMddHHmmssSSS000001
	用户编号：userId    XXXXXXXYY
	订单创建时间： ts    yyyyMMddHHmmssSSS
	订单金额：money
	订单状态：status  0：下单   1：付款   2：取消


create table kafka_source_order(
id string,
userId string,
money double,
status int,
ts string
)with(
'connector'='kafka',
'topic'='ordertopic',
'properties.bootstrap.servers'='hadoop000:9092',
'properties.group.id'='pk',
'scan.startup.mode'='latest-offset',
'json.fail-on-missing-field'='false',
'json.ignore-parse-errors'='true',
'format'='json'
);


CREATE TABLE hudi_order(
id string primary key not enforced,
userId string,
money double,
status int,
ts string,
dt string
)
PARTITIONED BY (`dt`)
WITH (
  'connector' = 'hudi',
  'path' = 'hdfs://hadoop000:8020/pk-hudi/hudi_order',
  'table.type' = 'MERGE_ON_READ',
  'write.tasks' = '1',
  'write.precombine.field'='ts'
);

insert into hudi_order select id,userId,money,status, ts, substring(ts,0,8) dt from kafka_source_order;


开发过程中：快速的命令中测试，最终可以达成代码的方式运行




